{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json ('drill_report_data_confidential.json')\n",
    "df.to_csv ('data_excel_confidential.csv', index = None)\n",
    "\n",
    "df1 = (df.groupby('file_no')['text']\n",
    "       .apply(lambda x: ','.join(x.dropna().unique()))\n",
    "       .reset_index())\n",
    "texty = df1['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texty)):\n",
    "    texty[i] = texty[i].replace(\"\\r\", \" \").replace(\"\\t\", \" \").replace(\"\\n\",\" \").replace('\"', \"\").replace(\"'\", \"\").replace('/',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import sys\n",
    "flag =1\n",
    "WL = WordNetLemmatizer()\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "StopWords = set(stopwords.words('english'))\n",
    "Filter_number_1 = []\n",
    "\n",
    "for text2 in texty:\n",
    "    FilterNumber1 = (re.sub('[^A-Za-z ]+', '', str(text2))).lower() ## To lower and Selecting only alphabets\n",
    "    filterNumber2 = nlp(FilterNumber1)\n",
    "    #Tokens = word_tokenize(FilterNumber1)\n",
    "    Filtered_Sentence = \"\"\n",
    "    for word in filterNumber2:\n",
    "        if word not in StopWords and len(word) > 1:\n",
    "            Filtered_Sentence = Filtered_Sentence + \" \" + word.lemma_ #(PS.stem(WL.lemmatize(word)))\n",
    "    Filter_number_1.append(Filtered_Sentence)\n",
    "    print(flag)\n",
    "    flag+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy import SymSpell\n",
    "import pkg_resources\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance = 3, prefix_length = 7)\n",
    "corpus_path = 'Unique_tokens_gloss.txt'\n",
    "sym_spell.create_dictionary(corpus_path)\n",
    "\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "\n",
    "if not sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
    "    print(\"Dictionary file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import sys\n",
    "Final_Array = []\n",
    "# lookup suggestions for multi-word input strings (supports compound splitting & merging)\n",
    "flag = 1725\n",
    "start = time.time()\n",
    "for ind_sent in from_1725:  \n",
    "    if len(ind_sent) > 5000:\n",
    "        final_ind_row = ''\n",
    "        #dd_sublen = round(len(ind_sent[0])/10)\n",
    "        Tokens = word_tokenize(ind_sent)\n",
    "        dd_sublen = round(len(Tokens)/20)\n",
    "        for i in range(20):\n",
    "            dd_sub = ''\n",
    "            Final_sentt = []\n",
    "            #dd_sub = ind_sent[0][i*dd_sublen:(i+1)*dd_sublen]\n",
    "            for j in Tokens[i*dd_sublen:(i+1)*dd_sublen]:\n",
    "                j+= \" \"\n",
    "                dd_sub += j\n",
    "            suggestions = sym_spell.lookup_compound(dd_sub, max_edit_distance = 2)\n",
    "            for suggestion in suggestions:\n",
    "                Final_sentt.append(suggestion)\n",
    "            final_ind_row += Final_sentt[0].term\n",
    "        Final_Array.append(final_ind_row)\n",
    "    else:\n",
    "        Final_sentt = []\n",
    "        suggestions = sym_spell.lookup_compound(ind_sent, max_edit_distance = 2)\n",
    "        for suggestion in suggestions:\n",
    "            Final_sentt.append(suggestion)\n",
    "        Final_Array.append(Final_sentt[0].term)\n",
    "    \n",
    "    print(flag)\n",
    "    flag += 1\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symspell_dict = [x for x in sym_spell.words.keys()]\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import sys\n",
    "\n",
    "After_spell_prepro = []\n",
    "\n",
    "StopWords = set(stopwords.words('english'))\n",
    "for doc in After_spell:\n",
    "    if type(doc[0]) is not float:\n",
    "        Tokens = word_tokenize(doc[0])\n",
    "        remove_stops = \"\"\n",
    "        for word in Tokens:\n",
    "            if word not in StopWords and len(word) > 1 and word in symspell_dict and word != 'pron':\n",
    "                remove_stops = remove_stops + \" \" + word#(PS.stem(WL.lemmatize(word)))\n",
    "    else:\n",
    "        remove_stops = ''\n",
    "    After_spell_prepro.append(remove_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy Docs removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag=0\n",
    "flag1=0\n",
    "short_docs = []\n",
    "FINAL_LIST = []\n",
    "After_filter_by_len_files = []\n",
    "for ind, doc in enumerate(After_spell_prepro):    \n",
    "    if type(doc[0]) == float or len(doc[0]) <= 100:\n",
    "        flag+=1\n",
    "        short_docs.append([file_number[ind],doc[0]])\n",
    "    elif(len(doc[0]) > 100):\n",
    "        flag1+=1\n",
    "        FINAL_LIST.append([file_number[ind],doc[0]])\n",
    "        \n",
    "print(\"Total docs: \", len(After_spell_prepro))        \n",
    "print(\"No chars: \", flag)\n",
    "print(\"<100 chars: \",flag1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=lesslerstore;AccountKey=8lmfbr9xmsPk3ZXhRLGveFoPyLtjJ05xkjOC6UN8jJfLBzivlld3C12awoUYSVdjIOl6Vt5Md2SD6a6CCTH13w==;EndpointSuffix=core.windows.net\"\n",
    "service = BlobServiceClient.from_connection_string(conn_str=connection_string)\n",
    "\n",
    "from azure.storage.blob import BlobClient\n",
    "import os\n",
    "filesss = ['2200','17411']\n",
    "for file_number in filesss:\n",
    "    #file_number = '16071'\n",
    "    file_name_list = []\n",
    "    for pag in range(1,50):\n",
    "        try:\n",
    "            blob = BlobClient.from_connection_string(conn_str=connection_string, container_name=\"lesslerfilestore/PDForiginal/File_No_\" + file_number + \"/images\", blob_name=\"File_No_\"+file_number+\"_PageNo_\"+str(pag))\n",
    "            file_name_list.append(\"page_\"+str(pag)+\".jpg\") \n",
    "            with open(os.path.join(r\"Raw_data_original\", \"page_\"+str(pag)+\".jpg\"), \"wb\") as my_blob:\n",
    "                blob_data = blob.download_blob()\n",
    "                blob_data.readinto(my_blob)\n",
    "        except:\n",
    "            print(\"Check directory for files\")\n",
    "            break\n",
    "\n",
    "    #------------------------------------------------------------\n",
    "\n",
    "    from fpdf import FPDF\n",
    "    pdf = FPDF()\n",
    "    # imagelist is the list with all image filenames\n",
    "    for image in file_name_list[:-1]:\n",
    "        pdf.add_page()\n",
    "        pdf.image(\"Raw_data_original/\"+image,0,0,210,297)\n",
    "    pdf.output(\"Raw_data_original/\"+file_number+\".pdf\", \"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "tokenized_text = []\n",
    "for j in data_:\n",
    "    tokenized_text.append(j[1].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(tokenized_text, min_count = 30) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[tokenized_text])\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "#print(trigram_mod[bigram_mod[tokenized_text[0]]])\n",
    "\n",
    "\n",
    "bi_tri_grams = []\n",
    "with_grams = [trigram_mod[bigram_mod[doc]] for doc in tokenized_text]\n",
    "\n",
    "for doc in with_grams:\n",
    "     #for token in trigram[bigram[tokenized_text[idx]]]:\n",
    "    for token in doc:\n",
    "        \n",
    "        if '_' in token:\n",
    "    #             tokenized_text[idx].append(token)\n",
    "            bi_tri_grams.append(token)\n",
    "            \n",
    "# len(bi_tri_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_could_dict=Counter(bi_tri_grams)\n",
    "wordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(word_could_dict)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.savefig('yourfile.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(sorted(word_could_dict.items(), key=lambda item: item[1], reverse = True)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "ngram_docs = []\n",
    "for tokens in with_grams:\n",
    "    ngram_docs.append(TreebankWordDetokenizer().detokenize(tokens))\n",
    "        \n",
    "        \n",
    "df_spell_s = pd.DataFrame(ngram_docs)\n",
    "df_spell_s.to_csv (r'Ngrams_docs.csv', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
